{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Concha.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "iViwBaEQTR4N"
      },
      "outputs": [],
      "source": [
        "# importing some useful libraries for the first part of project\n",
        "import numpy as np\n",
        "import scipy\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.multioutput import RegressorChain\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Paper3_WebData_Final.csv')\n",
        "print(df.shape)\n",
        "df = pd.read_csv('Paper3_WebData_Final.csv')\n",
        "df = df.drop(['test_date','nid','gender','naics', 'age_group','region','NAICS_descr'], axis=1)\n",
        "df_L = df.drop(['R500k','R1k','R2k','R3k', 'R4k', 'R6k', 'R8k'], axis=1)\n",
        "df_R = df.drop(['L500k','L1k','L2k','L3k', 'L4k', 'L6k', 'L8k'], axis=1)\n",
        "\n",
        "# Seperating left and right ears data so that we could use them for creating model.\n",
        "df_L.columns = ['500k','1k','2k','3k', '4k', '6k', '8k']\n",
        "df_R.columns = ['500k','1k','2k','3k', '4k', '6k', '8k']\n",
        "data = pd.concat([df_L, df_R], axis=0)\n",
        "data =data.reset_index(drop=True)\n",
        "# showing data\n",
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdJJtP76iYGu",
        "outputId": "44742e37-9954-4cb8-b3c4-dac61f4b6e44"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1831084, 21)\n",
            "(3776780, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaning data, removing nan values, and '**', and converting object type to int.\n",
        "data.isnull().sum()/data.shape[0]\n",
        "data = data.replace('**',np.nan)\n",
        "data = data.dropna()\n",
        "data.isnull().sum()\n",
        "data=data.astype(float)\n",
        "\n",
        "# showing data\n",
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3uMAsmB-gdT",
        "outputId": "60ee6378-05a8-427c-d8c2-829a55cbe7f6"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2567392, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = data.copy()\n",
        "df =df.drop_duplicates()\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F78tu3Zt-rZw",
        "outputId": "c7e42e0a-57b4-436d-e4d6-f6da15b25c63"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(661787, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on the previous notebook, I will predict '3k', and so on.( based on col_list), I chose this way based on heatmap correlation.\n",
        "print(df.columns)\n",
        "col_list = ['3k', '8k', '1k', '500k']\n",
        "print(col_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzG2OhIcjgvP",
        "outputId": "0f30b708-954e-4d30-9983-fd01b7533ffd"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['500k', '1k', '2k', '3k', '4k', '6k', '8k'], dtype='object')\n",
            "['3k', '8k', '1k', '500k']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a class in order to take care of data in each step, there are 4 steps.\n",
        "# Here with this class you could change the order of prediction, for instance you could have col_list= ['8k', '500k', '1k', '3k'], and\n",
        "# then predict '8k', and'500k', and so on\n",
        "class RemoveDuplicate():\n",
        "    d_list =[]\n",
        "    def __init__(self, data, col_list, column):\n",
        "        self.column = column\n",
        "        self.col_list = col_list\n",
        "        self.data = data\n",
        "    \n",
        "\n",
        "    def create_data(self):\n",
        "    \n",
        "        data1 = self.data.copy()\n",
        "        for d in self.col_list:\n",
        "            \n",
        "            #print(data1.shape)\n",
        "            df = data1.drop(d,axis=1)\n",
        "            df =df.drop_duplicates()\n",
        "            val_index = list(df.index.values.tolist())\n",
        "            self.d_list.append(val_index)\n",
        "            data1 =df\n",
        "        \n",
        "        if self.column == self.col_list[3]:\n",
        "          #print(len(self.d_list[0]))\n",
        "          dX = self.data.loc[self.d_list[0]]\n",
        "          X = dX.drop(columns=[self.col_list[3]], axis=1)\n",
        "          y = dX[self.col_list[3]]\n",
        "          X = X.to_numpy()\n",
        "          y = y.to_numpy()\n",
        "          y = y.flatten()\n",
        "          #print(X.shape,y.shape)\n",
        "          return X,y\n",
        "\n",
        "\n",
        "        if self.column == self.col_list[2]:\n",
        "          #print(len(self.d_list[1]))\n",
        "          dX = self.data.loc[self.d_list[1]]\n",
        "          X = dX.drop(columns=[self.col_list[3], self.col_list[2] ], axis=1)\n",
        "          y = dX[self.col_list[2]]\n",
        "          X = X.to_numpy()\n",
        "          y = y.to_numpy()\n",
        "          y = y.flatten()\n",
        "          #print(X.shape,y.shape)\n",
        "          \n",
        "          return X,y\n",
        "        \n",
        "        if self.column == self.col_list[1]:\n",
        "          #print(len(self.d_list[2]))\n",
        "          dX = self.data.loc[self.d_list[2]]\n",
        "          X = dX.drop(columns=[self.col_list[3], self.col_list[2], self.col_list[1] ], axis=1)\n",
        "          y = dX[self.col_list[1]]\n",
        "          X = X.to_numpy()\n",
        "          y = y.to_numpy()\n",
        "          y = y.flatten()\n",
        "          #print(X.shape,y.shape)\n",
        "          return X,y\n",
        "        \n",
        "        if self.column == self.col_list[0]:\n",
        "          #print(len(self.d_list[3]))\n",
        "          dX = self.data.loc[self.d_list[3]]\n",
        "          X = dX.drop(columns=[self.col_list[3], self.col_list[2], self.col_list[1], self.col_list[0] ], axis=1)\n",
        "          y = dX['3k']\n",
        "          X = X.to_numpy()\n",
        "          y = y.to_numpy()\n",
        "          y = y.flatten()\n",
        "          #print(X.shape,y.shape)\n",
        "          return X,y"
      ],
      "metadata": {
        "id": "V1M3kX2Ol23p"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For instance , if somebody wants to predict '3k', one should remove all three columns.\n",
        "# This class gives X, y datset for each step.\n",
        "RemoveDuplicate(df, col_list, '3k').create_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh0fpbUlMmUa",
        "outputId": "21e808b0-59da-44d7-ec08-0257bb6238be"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 5., 10.,  5.],\n",
              "        [ 5.,  5., 10.],\n",
              "        [20., 15., 20.],\n",
              "        ...,\n",
              "        [25., 90., 99.],\n",
              "        [45., 99., 99.],\n",
              "        [25., 65., 99.]]), array([15., 15., 20., ..., 85., 70., 50.]))"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to take care of model parameters, here I used Random Forest parameters\n",
        "\n",
        "def grid_Serch_pipeline(df, col_list, column, model, parameters):\n",
        "\n",
        "    X, y = RemoveDuplicate(df, col_list, column).create_data()\n",
        "   \n",
        "    grid_search = GridSearchCV(model, param_grid=parameters, n_jobs=1)\n",
        "    grid_search.fit(X, y)\n",
        "\n",
        "\n",
        "    grid_search.fit(X, y)\n",
        "    return grid_search.best_score_, grid_search.best_params__"
      ],
      "metadata": {
        "id": "xwn5g7ZMPm2H"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters= {\n",
        "    \"n_estimators\": [5, 10,50,100, 300, 500],\n",
        "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
        "    \"max_depth\": [1,5,10, 20, 50, 70, 80, 100, None],\n",
        "    \"bootstrap\": [True, False],\n",
        "    }\n",
        "\n",
        "model = RandomForestRegressor()"
      ],
      "metadata": {
        "id": "DSArrWkmDAP9"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy, parameters =grid_Serch_pipeline(df, col_list, '3k', model, parameters)\n"
      ],
      "metadata": {
        "id": "zP5iMrqmbs_5"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here someone could create the models, and save it for usage\n",
        "# Here this could take a long time to finish, becasue I put the grid search for each step in this part.\n",
        "# with this model you could ask more input values from a person, and predict the rest of gain frequnceis\n",
        "from joblib import Parallel, delayed\n",
        "import joblib\n",
        "\n",
        "col_list = ['3k', '8k', '1k', '500k']\n",
        "\n",
        "def ModelRF(col_list):\n",
        "    \n",
        "    models = []\n",
        "    # i = 1\n",
        "    for c, column in enumerate(col_list):\n",
        "    \n",
        "        accuracy, parameters =grid_Serch_pipeline(df, col_list, column)\n",
        "        model, X, y = build_pipeline(df, col_list, column, parameters)\n",
        "        model.fit(X,y)\n",
        "        joblib.dump(model, 'RandomForest+\"c\".pkl')\n",
        "        models.append(model)\n",
        "\n",
        "        # pred1 = model.predict(inputs)\n",
        "\n",
        "        # inputs=np.append(inputs,int(pred1))\n",
        "        # inputs = inputs.reshape(1, 3 + i)\n",
        "        # i +=1\n",
        "\n",
        "    # inputs = inputs.reshape(7,)\n",
        "    # outputs = np.array([inputs[6],  inputs[5], inputs[0], inputs[3], inputs[1], inputs[2], inputs[4]], dtype='int8')\n",
        "    \n",
        "    return models"
      ],
      "metadata": {
        "id": "BohZuT3f2Y8y"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here I tested the model in colab, this is the same previous model I wrote in jupyter, but with less code (creating some pipeline)\n",
        "import sys, os\n",
        "import time\n",
        "\n",
        "\n",
        "def resource_path(relative_path):\n",
        "    \"\"\" Get the absolute path to the resource, works for dev and for PyInstaller \"\"\"\n",
        "    try:\n",
        "        # PyInstaller creates a temp folder and stores path in _MEIPASS\n",
        "        base_path = sys._MEIPASS\n",
        "    except Exception:\n",
        "        base_path = os.path.abspath(\".\")\n",
        "\n",
        "    return os.path.join(base_path, relative_path)"
      ],
      "metadata": {
        "id": "3kJZ6PiKGKkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(resource_path(\"C:/Users/gjaha/pyproj/Take_Home_Project_ Machine_Learning/RandomForest0.pkl\"), \"rb\") as f:\n",
        "    forest1 = joblib.load(f)\n",
        "with open(resource_path(\"C:/Users/gjaha/pyproj/Take_Home_Project_ Machine_Learning/RandomForest1.pkl\"), \"rb\") as f1:\n",
        "    forest2 = joblib.load(f1)\n",
        "with open(resource_path(\"C:/Users/gjaha/pyproj/Take_Home_Project_ Machine_Learning/RandomForest2.pkl\"), \"rb\") as f2:\n",
        "    forest3 = joblib.load(f2)\n",
        "with open(resource_path(\"C:/Users/gjaha/pyproj/Take_Home_Project_ Machine_Learning/RandomForest3.pkl\"), \"rb\") as f3:\n",
        "    forest4 = joblib.load(f3)"
      ],
      "metadata": {
        "id": "kXQ1w80hG9tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input three data\n",
        "val_2k = int(input(\"Enter individuals hearing threshold gain value (in dB) at 2kHz: \"))\n",
        "val_4k = int(input(\"Enter individuals hearing threshold gain value (in dB) at 4kHz: \"))\n",
        "val_6k = int(input(\"Enter individuals hearing threshold gain value (in dB) at 6kHz: \"))"
      ],
      "metadata": {
        "id": "YLnATWMXHF3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs1 = np.array([val_2k, val_4k, val_6k], dtype='int8')\n",
        "out1 = forest1.predict([inputs1]).astype(int)\n",
        "inputs2 = np.array([val_2k, val_4k, val_6k, out1[0]], dtype='int8')\n",
        "out2 = forest2.predict([inputs2]).astype(int)\n",
        "inputs3 = np.array([val_2k, val_4k, val_6k, out1[0], out2[0]], dtype='int8')\n",
        "out3 = forest3.predict([inputs3]).astype(int)\n",
        "inputs4 = np.array([val_2k, val_4k, val_6k, out1[0], out2[0], out3[0]], dtype='int8')\n",
        "out4 = forest4.predict([inputs4]).astype(int)\n",
        "output = np.array([out4[0],  out3[0], val_2k, out1[0], val_4k, val_6k, out2[0] ], dtype='int8')\n",
        "output"
      ],
      "metadata": {
        "id": "REFccBlHFhRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ywgt-ahD7cNQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}